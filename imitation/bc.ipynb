{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DISPLAY=:1\n"
     ]
    }
   ],
   "source": [
    "# Notebook support or argpase\n",
    "import sys; sys.argv=['']; del sys\n",
    "%set_env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoboHive:> Registering Arms Envs\n",
      "RoboHive:> Registering Myo Envs\n",
      "RoboHive:> Registering Hand Envs\n",
      "RoboHive:> Registering Claw Envs\n",
      "RoboHive:> Registering Appliances Envs\n",
      "RoboHive:> Registering Multi-Task (2 subtasks) Envs\n",
      "RoboHive:> Registering FrankaKitchen (FK1) Envs\n",
      "RoboHive:> Registering Multi-Task (9 subtasks) Envs\n",
      "RoboHive:> Registering Quadruped Envs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random \n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary as th_summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset utils\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "# Robohive dependencies\n",
    "import gym\n",
    "from robohive.logger.grouped_datasets import Trace as RoboHive_Trace\n",
    "\n",
    "# Config and logging helpers\n",
    "import tools\n",
    "from configurator import generate_args, get_arg_dict\n",
    "from th_logger import TBXLogger as TBLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCIterableDataset(IterableDataset):\n",
    "  def __init__(self, dataset_path, scale_obs=False, seed=111):\n",
    "    self.seed = seed\n",
    "    self.dataset_path = dataset_path\n",
    "    self.scale_obs = scale_obs # From original range to [-1,1] by default\n",
    "\n",
    "    # Read episode filenames in the dataset path\n",
    "    self.ep_filenames = os.listdir(dataset_path)\n",
    "    # NOTE: buffering all trajectories might not be sustainable for larger datasets\n",
    "    # Consider lazy loading scheme instead\n",
    "    self.buffer = {\n",
    "      \"observations\": [],\n",
    "      \"actions\": [],\n",
    "      \"dones\": [],\n",
    "      \"target_positions\": []\n",
    "    }\n",
    "\n",
    "    for ep_filename in self.ep_filenames:\n",
    "      ep_fullpath = os.path.join(self.dataset_path, ep_filename)\n",
    "\n",
    "      # Read the Robohive trace\n",
    "      trace = RoboHive_Trace(\"\")\n",
    "      trace = trace.load(ep_fullpath)\n",
    "\n",
    "      ep_observations, ep_actions, ep_dones, ep_target_positions = \\\n",
    "        trace[\"Trial0\"].get(\"observations\"), \\\n",
    "        trace[\"Trial0\"].get(\"actions\"), \\\n",
    "        trace[\"Trial0\"].get(\"done\"), \\\n",
    "        trace[\"Trial0\"].get(\"target_pos\"), \\\n",
    "      \n",
    "      self.buffer[\"observations\"].append(ep_observations)\n",
    "      self.buffer[\"actions\"].append(ep_actions)\n",
    "      self.buffer[\"dones\"].append(ep_dones)\n",
    "      self.buffer[\"target_positions\"].append(ep_target_positions)\n",
    "    \n",
    "    for k, v in self.buffer.items():\n",
    "      self.buffer[k] = np.concatenate(v)\n",
    "\n",
    "    # Adjusting shapes\n",
    "    self.buffer[\"dones\"] = self.buffer[\"dones\"][:, None]\n",
    "\n",
    "    # Recover total sample number in the buffer\n",
    "    self.buffer_length = self.buffer[\"dones\"].shape[0]\n",
    "    # Recover the min, max for the observations\n",
    "    self.obs_min, self.obs_max = \\\n",
    "      self.buffer[\"observations\"].min(), self.buffer[\"observations\"].max()\n",
    "\n",
    "    # DEBUG\n",
    "    for k in [\"observations\", \"actions\", \"dones\", \"target_positions\"]:\n",
    "      print(f\" # DBG: Buffer {k} shape: {np.shape(self.buffer[k])}\")\n",
    "      print(f\" # DBG: Data range of {k}: {self.buffer[k].min(), self.buffer[k].max()}\")\n",
    "    \n",
    "    print(f\"\\nInitialized IterDataset with {len(self.ep_filenames)} episodes, totalling {self.buffer_length} steps.\\n\")\n",
    "  def __iter__(self):\n",
    "    while True:\n",
    "      idx = th.randint(0, self.buffer_length, [1])\n",
    "\n",
    "      # Scaling observation and target_positions\n",
    "      obs_list = self.buffer[\"observations\"][idx].astype(np.float32)\n",
    "      target_pos_list = self.buffer[\"target_positions\"][idx].astype(np.float32)\n",
    "      if self.scale_obs:\n",
    "        obs_list = self.normalize_observation(obs_list)\n",
    "        target_pos_list = self.normalize_observation(target_pos_list)\n",
    "\n",
    "      # observation, action, done, target_pos of a random step from the buffer\n",
    "      yield obs_list, \\\n",
    "            self.buffer[\"actions\"][idx].astype(np.float32), \\\n",
    "            self.buffer[\"dones\"][idx].astype(np.float32), \\\n",
    "            target_pos_list  \n",
    "  \n",
    "  # Used for observation normalization\n",
    "  @staticmethod\n",
    "  def _scale_field(a, old_min=0., old_max=1., new_min=-1, new_max=1.):\n",
    "    assert old_min < old_max, f\"Invalid scaling: old_min {old_min} >= old_max: {old_max}\"\n",
    "    assert new_min < new_max, f\"Invalid scaling: new_min {new_min} >= new_max: {new_max}\"\n",
    "    return ((a - old_min) * (new_max - new_min)) / (old_max - old_min) + new_min\n",
    "\n",
    "  def normalize_observation(self, x):\n",
    "    return self._scale_field(x, old_min=self.obs_min, old_max=self.obs_max)\n",
    "\n",
    "def make_dataloader(dataset_path, batch_size, scale_obs=False, seed=111, num_workers=2):\n",
    "  def worker_init_fn(worker_id):\n",
    "    # worker_seed = th.initial_seed() % (2 ** 32)\n",
    "    worker_seed = 133754134 + worker_id\n",
    "\n",
    "    random.seed(worker_seed)\n",
    "    np.random.seed(worker_seed)\n",
    "\n",
    "  th_seed_gen = th.Generator()\n",
    "  th_seed_gen.manual_seed(133754134 + seed)\n",
    "\n",
    "  dloader = iter(\n",
    "    DataLoader(\n",
    "      BCIterableDataset(dataset_path=dataset_path, scale_obs=scale_obs),\n",
    "        batch_size=batch_size, num_workers=num_workers,\n",
    "        worker_init_fn=worker_init_fn, generator=th_seed_gen)\n",
    "  )\n",
    "\n",
    "  return dloader\n",
    "\n",
    "# Eval helper\n",
    "@th.inference_mode()\n",
    "def eval_agent(env, agent, args, dataset=None):\n",
    "  \"\"\"\n",
    "    args: used to recover eval settings, and observation scaling\n",
    "    dataset: used to scale observations\n",
    "  \"\"\"\n",
    "  solved_list = []\n",
    "  video_dict = {}\n",
    "  n_video_saved = 0\n",
    "\n",
    "  for eval_ep_idx in range(args.eval_n_episodes):\n",
    "    obs = env.reset()\n",
    "    target_pos = env.get_target_pos()\n",
    "    obs_target = np.concatenate([obs, target_pos])\n",
    "    if args.scale_obs:\n",
    "      obs_target = dataset.normalize_observation(obs_target)\n",
    "    \n",
    "    solved = False\n",
    "    MAX_STEPS=500\n",
    "    t = 0\n",
    "    \n",
    "    if args.save_videos and n_video_saved < args.save_videos_n_max:\n",
    "      # NOTE: video collectin and saving is expensive process\n",
    "      ep_video_data = []\n",
    "\n",
    "    while not solved and t < MAX_STEPS:\n",
    "      action = agent(th.Tensor(obs_target)[None, :].float().to(agent.device))\n",
    "      action = action[0].cpu().numpy()\n",
    "\n",
    "      obs, _, _, info = env.step(action)\n",
    "      solved = info[\"solved\"]\n",
    "      obs_target = np.concatenate([obs, target_pos])\n",
    "      if args.scale_obs:\n",
    "        obs_target = dataset.normalize_observation(obs_target)\n",
    "\n",
    "      if args.save_videos and n_video_saved < args.save_videos_n_max:\n",
    "        ep_video_data.append(env.get_visuals()[\"rgb:front_cam:480x640:2d\"])\n",
    "\n",
    "      t += 1\n",
    "    \n",
    "    # Cummulate stats\n",
    "    solved_list.append(solved)\n",
    "    if args.save_videos and n_video_saved < args.save_videos_n_max:\n",
    "      video_dict[f\"eval_ep_{eval_ep_idx}\"] = \\\n",
    "        np.array(ep_video_data)[None, :].transpose(0, 1, 4, 2, 3)\n",
    "      n_video_saved += 1\n",
    "\n",
    "  return solved_list, video_dict\n",
    "\n",
    "# DEBUG:\n",
    "# Hyparam placeholders\n",
    "# dataset_path = \"../data/2024-01-30-pick-place-dataset/\"\n",
    "# batch_size = 32\n",
    "\n",
    "# Testing BCIterableDataset loading functions\n",
    "# iterds = BCIterableDataset(dataset_path=dataset_path, batch_size=batch_size)\n",
    "\n",
    "# Testing data loader\n",
    "# dataloader = make_dataloader(dataset_path, batch_size)\n",
    "\n",
    "# Load a batch:\n",
    "# obs_list, act_list, done_list, target_pos_list = [b for b in next(dataloader)]\n",
    "# # obs_list, act_list, done_list, target_pos_list\n",
    "# obs_list.shape, act_list.shape, done_list.shape, target_pos_list.shape\n",
    "\n",
    "# Can access states about the dataset that were collected\n",
    "# in the IterableDAtaset / buffer, namely for observation normalization.\n",
    "# dataloader._dataset.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_06.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_00.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_15.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_08.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_02.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_04.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_05.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_13.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_10.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_16.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_03.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_19.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_11.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_17.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_18.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_07.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_12.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_09.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_14.h5\n",
      "Reading: ../data/2024-01-30-pick-place-dataset/teleop_gamepad_traj_01.h5\n",
      " # DBG: Buffer observations shape: (8084, 37)\n",
      " # DBG: Data range of observations: (-15.93, 8.24)\n",
      " # DBG: Buffer actions shape: (8084, 9)\n",
      " # DBG: Data range of actions: (-1.0, 1.0)\n",
      " # DBG: Buffer dones shape: (8084, 1)\n",
      " # DBG: Data range of dones: (False, False)\n",
      " # DBG: Buffer target_positions shape: (8084, 3)\n",
      " # DBG: Data range of target_positions: (-0.258, 0.85)\n",
      "\n",
      "Initialized IterDataset with 20 episodes, totalling 8084 steps.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1684209/4038019670.py:37: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  self.buffer[k] = np.concatenate(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Logdir: ./logs/_seed_111__2024_01_31_19_40_52_950326.musashi\n",
      "\u001b[36m    RoboHive: A unified framework for robot learning | https://sites.google.com/view/robohive\n",
      "        Code: https://github.com/vikashplus/robohive/stargazers (add a star to support the project)\n",
      "    \u001b[0m\n",
      "DeterministicActor(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=40, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=9, bias=True)\n",
      "    (7): Identity()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BC Loss:  150016/ 500000: 0.1112:  30%|██▉       | 4678/15626 [04:06<00:47, 232.40it/s] "
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "# region: Generating additional hyparams\n",
    "CUSTOM_ARGS = [\n",
    "  # General hyper parameters\n",
    "  get_arg_dict(\"seed\", int, 111),\n",
    "  get_arg_dict(\"total-steps\", int, 500_000),\n",
    "  \n",
    "  # Behavior hyparams\n",
    "  get_arg_dict(\"dataset-path\", str, \"../data/2024-01-30-pick-place-dataset/\"),\n",
    "  get_arg_dict(\"batch-size\", int, 32),\n",
    "  get_arg_dict(\"lr\", float, 2.5e-4), # Learning rate\n",
    "  get_arg_dict(\"optim-wd\", float, 0), # weight decay for Adam optim\n",
    "  get_arg_dict(\"loss-type\", str, \"mse\", metatype=\"choice\",\n",
    "    choices=[\"mse\"]),\n",
    "  get_arg_dict(\"scale-obs\", bool, True, metatype=\"bool\"), # Scales obs to [-1,1] range\n",
    "\n",
    "  ## Actor network params\n",
    "  get_arg_dict(\"actor-type\", str, \"deter\", metatype=\"choice\",\n",
    "    choices=[\"deter\"]),\n",
    "  get_arg_dict(\"actor-hid-layers\", int, 3),\n",
    "  get_arg_dict(\"actor-hid-size\", int, 512),\n",
    "\n",
    "  # Eval protocol\n",
    "  # TODO: max horizon for the eval step, etc...\n",
    "  get_arg_dict(\"eval\", bool, True, metatype=\"bool\"),\n",
    "  get_arg_dict(\"eval-every\", int, int(5e4)), # Every X updates\n",
    "  get_arg_dict(\"eval-n-episodes\", int, 5),\n",
    "\n",
    "  # Logging params\n",
    "  get_arg_dict(\"save-videos\", bool, True, metatype=\"bool\"),\n",
    "  get_arg_dict(\"save-videos-n-max\", int, 1), # Max number of videos to log\n",
    "  get_arg_dict(\"save-model\", bool, True, metatype=\"bool\"),\n",
    "  get_arg_dict(\"save-model-every\", int, int(5e4)), # Every X frames || steps sampled\n",
    "  get_arg_dict(\"log-training-stats-every\", int, int(100)), # Every X model update\n",
    "  get_arg_dict(\"logdir-prefix\", str, \"./logs/\") # Overrides the default one\n",
    "]\n",
    "args = generate_args(CUSTOM_ARGS)\n",
    "# endregion: Generating additional hyparams\n",
    "\n",
    "# Seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "th.manual_seed(args.seed)\n",
    "th.cuda.manual_seed_all(args.seed)\n",
    "th.backends.cudnn.deterministic = args.torch_deterministic\n",
    "# th.backends.cudnn.benchmark = args.cudnn_benchmark\n",
    "\n",
    "# Load the dataset\n",
    "dataloader = make_dataloader(args.dataset_path, \n",
    "                             args.batch_size,\n",
    "                             scale_obs=args.scale_obs)\n",
    "\n",
    "# Set device as GPU\n",
    "device = tools.get_device(args) if (not args.cpu and th.cuda.is_available()) else th.device(\"cpu\")\n",
    "\n",
    "# Experiment logger\n",
    "tblogger = TBLogger(exp_name=args.exp_name, args=args)\n",
    "print(f\"# Logdir: {tblogger.logdir}\")\n",
    "\n",
    "should_log_training_stats = tools.Every(args.log_training_stats_every)\n",
    "should_eval = tools.Every(args.eval_every)\n",
    "should_save_model = tools.Every(args.save_model_every)\n",
    "\n",
    "# Environment instantiation\n",
    "if args.eval:\n",
    "  encoder_type = \"2d\"\n",
    "  img_res=\"480x640\"\n",
    "  # img_res=\"240x424\"\n",
    "  env = gym.make(\"rpFrankaPickPlaceData-v0\", \n",
    "                 randomize=True,\n",
    "                 visual_keys= [\n",
    "                  # customize the visual keys\n",
    "                  # TODO: review what kind of camear angles we want / need\n",
    "                  # \"rgb:left_cam:{}:{}\".format(img_res, encoder_type),\n",
    "                  # \"rgb:right_cam:{}:{}\".format(img_res, encoder_type),\n",
    "                  # \"rgb:top_cam:{}:{}\".format(img_res, encoder_type),\n",
    "                  \"rgb:front_cam:{}:{}\".format(img_res, encoder_type),\n",
    "                  # \"rgb:Franka_wrist_cam:{}:{}\".format(img_res, encoder_type),\n",
    "                  # \"d:left_cam:{}:{}\".format(img_res, encoder_type),\n",
    "                  # \"d:right_cam:{}:{}\".format(img_res, encoder_type),\n",
    "                  # \"d:top_cam:{}:{}\".format(img_res, encoder_type),\n",
    "                  # \"d:front_cam:{}:{}\".format(img_res, encoder_type),\n",
    "                ])\n",
    "else:\n",
    "  # TODO: create place holder observation and action spaces\n",
    "  pass\n",
    "\n",
    "# Agent models\n",
    "# TODO: separate to models.py in case we have more models\n",
    "class DeterministicActor(nn.Module):\n",
    "  def __init__(self,\n",
    "                input_dim,\n",
    "                output_dim,\n",
    "                n_layers,\n",
    "                hid_size,\n",
    "                act_fn=nn.ReLU,\n",
    "                out_act_fn=nn.Identity):\n",
    "    super().__init__()\n",
    "\n",
    "    network = []\n",
    "\n",
    "    for h0, h1 in zip(\n",
    "      [input_dim, *[hid_size for _ in range(n_layers)]],\n",
    "      [*[hid_size for _ in range(n_layers)], output_dim],\n",
    "      ):\n",
    "      network.extend([\n",
    "        nn.Linear(h0, h1),\n",
    "        act_fn()])\n",
    "    \n",
    "    network.pop()\n",
    "    network.append(out_act_fn())\n",
    "    \n",
    "    self.network = nn.Sequential(*network)\n",
    "\n",
    "    # TODO: init scehems\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # TODO: some asserts on the type and shape ?\n",
    "    return self.network(x)\n",
    "\n",
    "  def get_n_params(self):\n",
    "    return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "  def to(self, device):\n",
    "    super().to(device)\n",
    "    self.device = device\n",
    "    return self\n",
    "\n",
    "# Agent instantiation\n",
    "if args.actor_type == \"deter\":\n",
    "  agent = DeterministicActor(\n",
    "    37+3, # TODO soft code\n",
    "    9,\n",
    "    n_layers=args.actor_hid_layers,\n",
    "    hid_size=args.actor_hid_size).to(device)\n",
    "else:\n",
    "  raise NotImplementedError(f\"Unsupported agent type: {args.agent_type}\")\n",
    "\n",
    "# DBG: agent structure\n",
    "print(agent)\n",
    "th_summary(agent)\n",
    "\n",
    "# Optimizers\n",
    "optimizer = th.optim.Adam(agent.parameters(),\n",
    "                          lr=args.lr,\n",
    "                          eps=1e-5,\n",
    "                          weight_decay=args.optim_wd)\n",
    "\n",
    "# Training start\n",
    "start_time = time.time()\n",
    "# Log the number of parameters of the model\n",
    "tblogger.log_stats({\n",
    "    \"n_params\": agent.get_n_params()\n",
    "}, 0, \"info\")\n",
    "\n",
    "# Training loop\n",
    "for global_step in (pbar := tqdm(range(0, args.total_steps + args.batch_size, args.batch_size))):\n",
    "  obs_list, act_list, done_list, target_pos_list = \\\n",
    "    [b.to(device) for b in next(dataloader)]\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  obs_target_pos_list = th.cat([\n",
    "    obs_list, target_pos_list], dim=1)\n",
    "\n",
    "  actions = agent(obs_target_pos_list)\n",
    "\n",
    "  bc_loss = F.mse_loss(actions, act_list)\n",
    "  bc_loss.backward()\n",
    "\n",
    "  optimizer.step()\n",
    "\n",
    "  if should_log_training_stats(global_step):\n",
    "    # print(f\"Step {global_step} / {args.total_steps}\")\n",
    "    # print(f\"  bc_loss: {bc_loss.item(): 0.3f}\")\n",
    "    pbar.set_description(f\"BC Loss: {global_step:7d}/{args.total_steps:7d}: {bc_loss.item():.4f}\")\n",
    "\n",
    "    # Training stats\n",
    "    train_stats = {\n",
    "      \"bc_loss\": bc_loss.item()\n",
    "    }\n",
    "    tblogger.log_stats(train_stats, global_step, prefix=\"train\")\n",
    "\n",
    "    # Info stats\n",
    "    info_stats = {\n",
    "      \"global_step\": global_step,\n",
    "      \"duration\": time.time() - start_time,\n",
    "      \"fps\": tblogger.track_duration(\"fps\", global_step),\n",
    "      \"env_step_duration\": tblogger.track_duration(\"fps_inv\", global_step, inverse=True),\n",
    "    }\n",
    "    tblogger.log_stats(info_stats, global_step, \"info\")\n",
    "  \n",
    "  if args.eval and global_step > 0 and should_eval(global_step):\n",
    "    eval_solved_list, eval_video_dict = eval_agent(env, agent, args, dataset=dataloader._dataset)\n",
    "    tblogger.log_stats({\n",
    "      \"success_rate\": np.mean(eval_solved_list),\n",
    "    }, global_step, prefix=\"eval\")\n",
    "    for k, v in eval_video_dict.items():\n",
    "      tblogger.log_video(k, v, global_step, fps=24, prefix=\"video\")\n",
    "\n",
    "  if args.save_model and should_save_model(global_step):\n",
    "    model_save_dir = tblogger.get_models_savedir()\n",
    "    model_save_name = f\"agent.{global_step}.ckpt.pth\"\n",
    "    model_save_fullpath = os.path.join(model_save_dir, model_save_name)\n",
    "\n",
    "    th.save(agent.state_dict(), model_save_fullpath)\n",
    "\n",
    "# Clean up\n",
    "tblogger.close()\n",
    "if args.eval:\n",
    "  env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_solved_list, eval_video_dict = eval_agent(env, agent, args, dataset=dataloader._dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['time', 'rgb:left_cam:240x424:2d', 'd:left_cam:240x424:2d', 'rgb:right_cam:240x424:2d', 'd:right_cam:240x424:2d', 'rgb:top_cam:240x424:2d', 'd:top_cam:240x424:2d', 'rgb:Franka_wrist_cam:240x424:2d', 'd:Franka_wrist_cam:240x424:2d'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_visuals().keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robohive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
